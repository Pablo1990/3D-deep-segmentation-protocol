{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pablo1990/3D-deep-segmentation-protocol/blob/main/3D_deep_segmentation_protocol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7c7V4yEqDc_"
      },
      "source": [
        "# 3D deep segmentation protocol\n",
        "\n",
        "Mount your google drive to access all your image files, segmentations, and custom models. This also ensures that any models you train are saved to your google drive. If you'd like to try out the notebook without your own files, please download the sample images from tissuenet (optional step in Setup below).\n",
        "\n",
        "This notebook was inspired by Cellpose 2.0 notebook (https://github.com/MouseLand/cellpose) and the Zero-Cost Deep-Learning to Enhance Microscopy project (https://github.com/HenriquesLab/DeepLearning_Collab/wiki). Jointly developed by the Jacquemet (link to https://cellmig.org/) and Henriques (https://henriqueslab.github.io/) laboratories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvyuR08OZfw4"
      },
      "source": [
        "# 0. Installation setup\n",
        "\n",
        "We will first install cellpose 2.0, check the GPU is working, and mount google drive to get your models and images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbqFni8kuFar"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUfSFlZgp1aV"
      },
      "source": [
        "Install cellpose -- by default the torch GPU version is installed in COLAB notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlMnqge-lQ9s"
      },
      "outputs": [],
      "source": [
        "!pip install \"opencv-python-headless<4.3\"\n",
        "!pip install cellpose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2cBEO1PLuO7"
      },
      "source": [
        "Check CUDA version and that GPU is working in cellpose and import other libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt8hgC7rniP8"
      },
      "outputs": [],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "\n",
        "import os, shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from cellpose import core, utils, io, models, metrics\n",
        "from glob import glob\n",
        "\n",
        "use_GPU = core.use_gpu()\n",
        "yn = ['NO', 'YES']\n",
        "print(f'>>> GPU activated? {yn[use_GPU]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heEiTSWQZZ6y"
      },
      "source": [
        "## Mount google drive\n",
        "\n",
        "Please mount your google drive and find your working folder with (if available) the model that you trained. If you want to train a model, create a folder in google drive with the images and the labels, either `_seg.npy` files from the cellpose gui, or `_masks.tif` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uGUNrjdRfVDs"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@markdown ###Run this cell to connect your Google Drive to Colab\n",
        "\n",
        "#@markdown * Click on the URL.\n",
        "\n",
        "#@markdown * Sign in your Google Account.\n",
        "\n",
        "#@markdown * Copy the authorization code.\n",
        "\n",
        "#@markdown * Enter the authorization code.\n",
        "\n",
        "#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\".\n",
        "\n",
        "#mounts user's Google Drive to Google Colab.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1Ta76yatmjH"
      },
      "source": [
        "### Download sample images (optional)\n",
        "\n",
        "If you don't mount your google drive, and want to test cellpose 2.0, run the next code block to download the example data. This `human_in_the_loop` folder has a `train` folder with training images and manual segmentations (in this case created in the loop), and a `test` folder with test images and manual segmentations from scratch.\n",
        "\n",
        "These images are described [here](https://www.ebi.ac.uk/bioimage-archive/galleries/S-BIAD843-ai.html). EXPLAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC7BIPW06z4t"
      },
      "outputs": [],
      "source": [
        "# !rm -rf labelled_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMG3YYFSdieb"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "from natsort import natsorted\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1gXmuJlNYXLxAZypwEb2dhOs0t9LxL04p'\n",
        "gdown.download(url, 'labelled_data.tar.gz', quiet=False)\n",
        "\n",
        "!tar -xzvf labelled_data.tar.gz\n",
        "!rm labelled_data.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Initial segmentation: Cellpose\n",
        "\n",
        "Cellpose is a deep learning algorithm that can segment cells in 2D and 3D.\n",
        "\n",
        "Explain the different Cellpose models:\n",
        "\n",
        "-\n"
      ],
      "metadata": {
        "id": "R7Zz3cKE6UMG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vDu4Ixjo588O"
      },
      "outputs": [],
      "source": [
        "# model name and path\n",
        "\n",
        "# model name and path\n",
        "#@markdown ###Name of the pretrained model to start from and new model name:\n",
        "from cellpose import models\n",
        "initial_model = \"cyto3\" #@param [\"cyto\", \"cyto3\",\"nuclei\",\"tissuenet_cp3\", \"livecell_cp3\", \"yeast_PhC_cp3\", \"yeast_BF_cp3\", \"bact_phase_cp3\", \"bact_fluor_cp3\", \"deepbacs_cp3\", \"scratch\"]\n",
        "model_name = \"CP_tissuenet\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ###Path to images:\n",
        "\n",
        "dir = \"init_segmentation\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ###Channel Parameters:\n",
        "\n",
        "Channel_to_use_for_segmentation = \"Grayscale\" #@param [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
        "\n",
        "# Here we match the channel to number\n",
        "if Channel_to_use_for_segmentation == \"Grayscale\":\n",
        "  chan = 0\n",
        "elif Channel_to_use_for_segmentation == \"Blue\":\n",
        "  chan = 3\n",
        "elif Channel_to_use_for_segmentation == \"Green\":\n",
        "  chan = 2\n",
        "elif Channel_to_use_for_segmentation == \"Red\":\n",
        "  chan = 1\n",
        "\n",
        "#@markdown ### Segmentation parameters:\n",
        "\n",
        "#@markdown diameter of cells (set to zero to use diameter from training set):\n",
        "diameter =  0#@param {type:\"number\"}\n",
        "#@markdown threshold on flow error to accept a mask (set higher to get more cells, e.g. in range from (0.1, 3.0), OR set to 0.0 to turn off so no cells discarded):\n",
        "flow_threshold = 0.4 #@param {type:\"slider\", min:0.0, max:3.0, step:0.1}\n",
        "#@markdown threshold on cellprob output to seed cell masks (set lower to include more pixels or higher to include fewer, e.g. in range from (-6, 6)):\n",
        "cellprob_threshold=0 #@param {type:\"slider\", min:-6, max:6, step:1}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gets image files in dir (ignoring image files ending in _masks)\n",
        "files = io.get_image_files(dir, '_masks')\n",
        "print(files)\n",
        "images = [io.imread(f) for f in files]\n",
        "\n",
        "# declare model\n",
        "model = models.CellposeModel(gpu=True,\n",
        "                             pretrained_model=model_path)\n",
        "\n",
        "# use model diameter if user diameter is 0\n",
        "diameter = model.diam_labels if diameter==0 else diameter\n",
        "\n",
        "# run model on test images\n",
        "masks, flows, styles = model.eval(images,\n",
        "                                  channels=[chan, chan2],\n",
        "                                  diameter=diameter,\n",
        "                                  flow_threshold=flow_threshold,\n",
        "                                  cellprob_threshold=cellprob_threshold\n",
        "                                  )"
      ],
      "metadata": {
        "id": "iCbh-3ut7Spf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Improving raw images for segmentation: Denoising\n",
        "A very common approach to improve classical segmentation algorithms is to denoise the images. This is done by applying a filter to the images that removes the noise while preserving the structures of interest.\n",
        "We have tried different denoising algorithms, but none of them improved the segmentation results.\n",
        "\n",
        "### Deconvolution\n",
        "TODO: GET SAMPLE DATA DECONVOLVED OR DECONVOLVE IT"
      ],
      "metadata": {
        "id": "35eTukvj_HSv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DdadkLXIZOZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Refining the segmentation: Cellpose fine-tuning\n",
        "Cellpose fine-tuning is a feature that allows you to improve the segmentation results by training a new model on your own data."
      ],
      "metadata": {
        "id": "L7DxZhik4aQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From 3D images to 2D images"
      ],
      "metadata": {
        "id": "iL7nKT4u5cPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code developed by Alvaro Miranda de Larra and Pablo Vicente-Munuera\n",
        "import os\n",
        "import tifffile as tiff\n",
        "\n",
        "def iterative_image_splicer(input_dir, output_dir, segmented_input=False):\n",
        "\n",
        "  # Create the output directory if it doesn't exist\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "  # Get a list of all .tif files in the input directory\n",
        "  tif_files = [f for f in os.listdir(input_dir) if f.endswith('.tif')]\n",
        "\n",
        "  # Iterate over each 3D image\n",
        "  for tif_file in tif_files:\n",
        "    if not tif_file.startswith('.'):\n",
        "      # Load the multi-directory TIFF image\n",
        "      with tiff.TiffFile(os.path.join(input_dir, tif_file)) as tif:\n",
        "          image = tif.asarray()\n",
        "\n",
        "      # Get the shape of the 3D image\n",
        "      z, y, x = image.shape\n",
        "      print(image.shape)\n",
        "      # Generate 2D images along XY, XZ, and YZ coordinates\n",
        "\n",
        "      for z_coord in range(z):\n",
        "          xy_image = image[z_coord, :, :]  # XY plane at the current Z coordinate\n",
        "\n",
        "          # Save the 2D images with appropriate names\n",
        "          base_name = os.path.splitext(tif_file)[0]\n",
        "          # Remove '_segmented' from base_name\n",
        "          base_name = base_name.replace('_segmented', '')\n",
        "          if segmented_input:\n",
        "            tiff.imwrite(os.path.join(output_dir, f'{base_name}_XY_Z{z_coord}_masks.tif'), xy_image)\n",
        "          else:\n",
        "            tiff.imwrite(os.path.join(output_dir, f'{base_name}_XY_Z{z_coord}.tif'), xy_image)\n",
        "\n",
        "      for xy_coord in range(x):\n",
        "          xz_image = image[:, :, xy_coord]  # XZ plane at the current Y coordinate\n",
        "          yz_image = image[:, xy_coord, :]  # YZ plane at the current X coordinate\n",
        "\n",
        "          if segmented_input:\n",
        "            tiff.imwrite(os.path.join(output_dir, f'{base_name}_XZ_Y{xy_coord}_masks.tif'), xz_image)\n",
        "            tiff.imwrite(os.path.join(output_dir, f'{base_name}_YZ_X{xy_coord}_masks.tif'), yz_image)\n",
        "          else:\n",
        "            tiff.imwrite(os.path.join(output_dir, f'{base_name}_XZ_Y{xy_coord}.tif'), xz_image)\n",
        "            tiff.imwrite(os.path.join(output_dir, f'{base_name}_YZ_X{xy_coord}.tif'), yz_image)\n",
        "\n",
        "!rm -rf labelled_data_2D/\n",
        "iterative_image_splicer('labelled_data/raw/', 'labelled_data_2D')\n",
        "iterative_image_splicer('labelled_data/segmented/', 'labelled_data_2D', segmented_input=True)"
      ],
      "metadata": {
        "id": "z1qBJXePXD9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j4m33Fuw5vm"
      },
      "source": [
        "what the training images look like + their labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldNwr_zxMVha"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from natsort import natsorted\n",
        "from glob import glob\n",
        "\n",
        "plt.figure(figsize=(12,4), dpi=300)\n",
        "count = 0\n",
        "\n",
        "train_files = natsorted([f for f in glob('labelled_data_2D/*.tif')\n",
        "                        if '_segmented_' not in f])\n",
        "train_seg = natsorted(glob('labelled_data_2D/*_segmented*.tif'))\n",
        "\n",
        "for k,f in enumerate(train_files):\n",
        "    img = io.imread(f)\n",
        "    plt.subplot(3,len(train_files), k+1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    if k==0:\n",
        "        plt.title('image')\n",
        "\n",
        "    if k>3:\n",
        "      break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split into training and set test\n"
      ],
      "metadata": {
        "id": "CKOhEhZHkVpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf test/\n",
        "!rm -rf train/\n",
        "\n",
        "# Divide sets into training and set test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_files = natsorted([f for f in glob('labelled_data_2D/*.tif')\n",
        "                        if '_masks.tif' not in f])\n",
        "train_files, test_files = train_test_split(train_files, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save files from train into 'train'\n",
        "os.makedirs('train', exist_ok=True)\n",
        "for f in train_files:\n",
        "    shutil.copy(f, 'train')\n",
        "    # Get the '_mask' from the file f\n",
        "    shutil.copy(f.replace('.tif', '_masks.tif'), 'train')\n",
        "\n",
        "# Save files from test into 'test'\n",
        "os.makedirs('test', exist_ok=True)\n",
        "for f in test_files:\n",
        "    shutil.copy(f, 'test')\n",
        "    # Get the '_mask' from the file f\n",
        "    shutil.copy(f.replace('.tif', '_masks.tif'), 'test')"
      ],
      "metadata": {
        "id": "AAbu2H2Xh8Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfE75htF0l84"
      },
      "source": [
        "### Train model on manual annotations\n",
        "\n",
        "Skip this step if you already have a pretrained model.\n",
        "\n",
        "Fill out the form below with the paths to your data and the parameters to start training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLdKNWQ4jxy5"
      },
      "source": [
        "### Training parameters\n",
        "\n",
        "<font size = 4> **Paths for training, predictions and results**\n",
        "\n",
        "\n",
        "<font size = 4>**`train_dir:`, `test_dir`:** These are the paths to your folders train_dir (with images and masks of training images) and test_dir (with images and masks of test images). You can leave the test_dir blank, but it's recommended to have some test images to check the model's performance. To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n",
        "\n",
        "<font size = 4>**`initial_model`:** Choose a model from the cellpose [model zoo](https://cellpose.readthedocs.io/en/latest/models.html#model-zoo) to start from.\n",
        "\n",
        "<font size = 4>**`model_name`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
        "\n",
        "<font size = 4>**Training parameters**\n",
        "\n",
        "<font size = 4>**`number_of_epochs`:** Input how many epochs (rounds) the network will be trained. At least 100 epochs are recommended, but sometimes 250 epochs are necessary, particularly from scratch. **Default value: 100**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQI4aUxCjz3n"
      },
      "outputs": [],
      "source": [
        "#@markdown ###Path to images and masks:\n",
        "\n",
        "train_dir = \"train\" #@param {type:\"string\"}\n",
        "test_dir = \"test\" #@param {type:\"string\"}\n",
        "#Define where the patch file will be saved\n",
        "base = \"/content\"\n",
        "\n",
        "# model name and path\n",
        "#@markdown ###Name of the pretrained model to start from and new model name:\n",
        "from cellpose import models\n",
        "initial_model = \"cyto3\" #@param [\"cyto\", \"cyto3\",\"nuclei\",\"tissuenet_cp3\", \"livecell_cp3\", \"yeast_PhC_cp3\", \"yeast_BF_cp3\", \"bact_phase_cp3\", \"bact_fluor_cp3\", \"deepbacs_cp3\", \"scratch\"]\n",
        "model_name = \"CP_tissuenet\" #@param {type:\"string\"}\n",
        "\n",
        "# other parameters for training.\n",
        "#@markdown ###Training Parameters:\n",
        "#@markdown Number of epochs:\n",
        "n_epochs =  100#@param {type:\"number\"}\n",
        "\n",
        "Channel_to_use_for_training = \"Grayscale\" #@param [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
        "\n",
        "# @markdown ###If you have a secondary channel that can be used for training, for instance nuclei, choose it here:\n",
        "\n",
        "Second_training_channel= \"None\" #@param [\"None\", \"Blue\", \"Green\", \"Red\"]\n",
        "\n",
        "\n",
        "#@markdown ###Advanced Parameters\n",
        "\n",
        "Use_Default_Advanced_Parameters = True #@param {type:\"boolean\"}\n",
        "#@markdown ###If not, please input:\n",
        "learning_rate = 0.1 #@param {type:\"number\"}\n",
        "weight_decay = 0.0001 #@param {type:\"number\"}\n",
        "\n",
        "if (Use_Default_Advanced_Parameters):\n",
        "  print(\"Default advanced parameters enabled\")\n",
        "  learning_rate = 0.1\n",
        "  weight_decay = 0.0001\n",
        "\n",
        "#here we check that no model with the same name already exist, if so delete\n",
        "model_path = train_dir + 'models/'\n",
        "if os.path.exists(model_path+'/'+model_name):\n",
        "  print(\"!! WARNING: \"+model_name+\" already exists and will be deleted in the following cell !!\")\n",
        "\n",
        "if len(test_dir) == 0:\n",
        "  test_dir = None\n",
        "\n",
        "# Here we match the channel to number\n",
        "if Channel_to_use_for_training == \"Grayscale\":\n",
        "  chan = 0\n",
        "elif Channel_to_use_for_training == \"Blue\":\n",
        "  chan = 3\n",
        "elif Channel_to_use_for_training == \"Green\":\n",
        "  chan = 2\n",
        "elif Channel_to_use_for_training == \"Red\":\n",
        "  chan = 1\n",
        "\n",
        "\n",
        "if Second_training_channel == \"Blue\":\n",
        "  chan2 = 3\n",
        "elif Second_training_channel == \"Green\":\n",
        "  chan2 = 2\n",
        "elif Second_training_channel == \"Red\":\n",
        "  chan2 = 1\n",
        "elif Second_training_channel == \"None\":\n",
        "  chan2 = 0\n",
        "\n",
        "if initial_model=='scratch':\n",
        "  initial_model = 'None'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8SDv9XztBgb"
      },
      "source": [
        "Here's what the command to train would be on the command line -- make sure if you run this locally to correct the paths for your local computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQsv-Iz7m_CF"
      },
      "outputs": [],
      "source": [
        "run_str = f'python -m cellpose --use_gpu --verbose --train --dir {train_dir} --pretrained_model {initial_model} --chan {chan} --chan2 {chan2} --n_epochs {n_epochs} --learning_rate {learning_rate} --weight_decay {weight_decay}'\n",
        "if test_dir is not None:\n",
        "    run_str += f' --test_dir {test_dir}'\n",
        "#run_str += ' --mask_filter _seg.npy' # if you want to use _seg.npy files for training\n",
        "print(run_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JRxBPmatrK7"
      },
      "source": [
        "### Train new model\n",
        "\n",
        "Using settings from form above, train model in notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcYskYudMajM"
      },
      "outputs": [],
      "source": [
        "from cellpose import train\n",
        "\n",
        "# start logger (to see training across epochs)\n",
        "logger = io.logger_setup()\n",
        "\n",
        "# DEFINE CELLPOSE MODEL (without size model)\n",
        "model = models.CellposeModel(gpu=use_GPU, model_type=initial_model)\n",
        "\n",
        "# get files\n",
        "output = io.load_train_test_data(train_dir, test_dir, mask_filter='')\n",
        "train_data, train_labels, _, test_data, test_labels, _ = output\n",
        "\n",
        "(new_model_path, train_losses, test_losses) = train.train_seg(model.net, train_data=train_data,\n",
        "                              train_labels=train_labels,\n",
        "                              test_data=test_data,\n",
        "                              test_labels=test_labels,\n",
        "                              save_path=train_dir,\n",
        "                              n_epochs=n_epochs,\n",
        "                              learning_rate=learning_rate,\n",
        "                              weight_decay=weight_decay,\n",
        "                              SGD=True,\n",
        "                              nimg_per_epoch=8,\n",
        "                              model_name=model_name)\n",
        "\n",
        "# diameter of labels in training images\n",
        "diam_labels = model.net.diam_labels.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdH0j8-L6FuB"
      },
      "source": [
        "## Evaluate on test data (optional)\n",
        "\n",
        "If you have test data, check performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0AGsH5p6K6S"
      },
      "outputs": [],
      "source": [
        "# get files (during training, test_data is transformed so we will load it again)\n",
        "output = io.load_train_test_data(test_dir, mask_filter='')\n",
        "test_data, test_labels = output[:2]\n",
        "\n",
        "# run model on test images\n",
        "masks = model.eval(test_data,\n",
        "                   diameter=diam_labels)[0]\n",
        "\n",
        "# check performance using ground truth labels\n",
        "ap = metrics.average_precision(test_labels, masks)[0]\n",
        "\n",
        "print(f'>>> average precision at iou threshold 0.5 = {ap[:,0].mean():.3f}')\n",
        "print(f'>>> average precision at iou threshold 0.75 = {ap[:,1].mean():.3f}')\n",
        "print(f'>>> average precision at iou threshold 0.95 = {ap[:,2].mean():.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masks"
      ],
      "metadata": {
        "id": "E8Xg8fUYW3Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2ac5gtr-HPq"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(12,8), dpi=150)\n",
        "for k,im in enumerate(test_data):\n",
        "    img = im.copy()\n",
        "    plt.subplot(3,len(train_files), k+1)\n",
        "    img = np.vstack((img, np.zeros_like(img)[:1]))\n",
        "    img = img.transpose(1,2,0)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    if k==0:\n",
        "        plt.title('image')\n",
        "\n",
        "    plt.subplot(3,len(train_files), len(train_files) + k+1)\n",
        "    plt.imshow(masks[k])\n",
        "    plt.axis('off')\n",
        "    if k==0:\n",
        "        plt.title('predicted labels')\n",
        "\n",
        "    plt.subplot(3,len(train_files), 2*len(train_files) + k+1)\n",
        "    plt.imshow(test_labels[k])\n",
        "    plt.axis('off')\n",
        "    if k==0:\n",
        "        plt.title('true labels')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Segmentation evaluation metrics: biology-based\n",
        "Common metrics used to evaluate the quality of a segmentation are: IoU (Intersection over Union), Dice coefficient, ... However, these metrics are not always suitable for biological images. We have developed a new metric that is more biologically relevant. It also helped us to improve the segmentation results from our manual annotations."
      ],
      "metadata": {
        "id": "bfKv4jgzAR9K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5LwS0FwAcw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Tracking cells in 3D: custom stitching method\n",
        "We still had issues with our segmentation results, especially when cells were touching each other. Therefore, we used TrackMate to track cells in 3D and then stitched the cells together. This allowed us to obtain a more accurate segmentation of the cells."
      ],
      "metadata": {
        "id": "3-up0gcY_a9O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cFoZgtFrAdaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbVIZbNk5hgR"
      },
      "source": [
        "# Use custom model to segment images\n",
        "\n",
        "Take custom trained model from above, or upload your own model to google drive / colab runtime.\n",
        "\n",
        "## Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axg2YQEpDx0e"
      },
      "source": [
        "if you're using the example test data we'll copy it to a new folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InyKGtD3D2ZX"
      },
      "outputs": [],
      "source": [
        "src = 'human_in_the_loop/test'\n",
        "if dir[:len(src)] == src:\n",
        "    files = io.get_image_files(dir, '_masks')\n",
        "    dir = 'human_in_the_loop/eval/'\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "    for f in files:\n",
        "        dst = dir + os.path.split(f)[1]\n",
        "        print(f'{f} > {dst}')\n",
        "        shutil.copyfile(f, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JJ1q0nTBAAR"
      },
      "source": [
        "Here's what the command to train would be on the command line -- make sure if you run this locally to correct the paths for your local computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P5voZOVM-H9"
      },
      "outputs": [],
      "source": [
        "run_str = f'python -m cellpose --use_gpu --verbose --dir {dir} --pretrained_model {model_path} --chan {chan} --chan2 {chan2} --diameter {diameter} --flow_threshold {flow_threshold} --cellprob_threshold {cellprob_threshold}'\n",
        "print(run_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN3rdsfMBc_8"
      },
      "source": [
        "## run custom model\n",
        "\n",
        "how to run the custom model in a notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCcbs722BYd0"
      },
      "outputs": [],
      "source": [
        "# gets image files in dir (ignoring image files ending in _masks)\n",
        "files = io.get_image_files(dir, '_masks')\n",
        "print(files)\n",
        "images = [io.imread(f) for f in files]\n",
        "\n",
        "# declare model\n",
        "model = models.CellposeModel(gpu=True,\n",
        "                             pretrained_model=model_path)\n",
        "\n",
        "# use model diameter if user diameter is 0\n",
        "diameter = model.diam_labels if diameter==0 else diameter\n",
        "\n",
        "# run model on test images\n",
        "masks, flows, styles = model.eval(images,\n",
        "                                  channels=[chan, chan2],\n",
        "                                  diameter=diameter,\n",
        "                                  flow_threshold=flow_threshold,\n",
        "                                  cellprob_threshold=cellprob_threshold\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj5AIZ825o7P"
      },
      "source": [
        "## save output to *_seg.npy\n",
        "\n",
        "you will see the files save in the Files tab and you can download them from there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc7EWe_f5oEH"
      },
      "outputs": [],
      "source": [
        "from cellpose import io\n",
        "\n",
        "io.masks_flows_to_seg(images,\n",
        "                      masks,\n",
        "                      flows,\n",
        "                      files,\n",
        "                      channels=[chan, chan2],\n",
        "                      diams=diameter*np.ones(len(masks)),\n",
        "                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwoUuuarC9V5"
      },
      "source": [
        "## save output masks to tiffs/pngs or txt files for imageJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da-Rtx09DEZB"
      },
      "outputs": [],
      "source": [
        "io.save_masks(images,\n",
        "              masks,\n",
        "              flows,\n",
        "              files,\n",
        "              channels=[chan, chan2],\n",
        "              png=True, # save masks as PNGs and save example image\n",
        "              tif=True, # save masks as TIFFs\n",
        "              save_txt=True, # save txt outlines for ImageJ\n",
        "              save_flows=False, # save flows as TIFFs\n",
        "              save_outlines=False, # save outlines as TIFFs\n",
        "              save_mpl=True # make matplotlib fig to view (WARNING: SLOW W/ LARGE IMAGES)\n",
        "              )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiP9MWN4F3Sx"
      },
      "outputs": [],
      "source": [
        "f = files[0]\n",
        "plt.figure(figsize=(12,4), dpi=300)\n",
        "plt.imshow(io.imread(os.path.splitext(f)[0] + '_cp_output.png'))\n",
        "plt.axis('off')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IvyuR08OZfw4",
        "VbqFni8kuFar",
        "heEiTSWQZZ6y"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}