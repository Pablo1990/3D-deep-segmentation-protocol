{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pablo1990/3D-deep-segmentation-protocol/blob/main/3D_deep_segmentation_protocol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7c7V4yEqDc_"
      },
      "source": [
        "# 3D deep segmentation protocol\n",
        "#### By Paci and Vicente-Munuera et al., 2025 https://arxiv.org/abs/2501.19203\n",
        "\n",
        "Here, we explain the 3D segmentation protocol for deep tissues (especifically, the *Drosophila* wing disc) with the following steps:\n",
        "0.   [Installation setup](#scrollTo=IvyuR08OZfw4)\n",
        "1.   [Initial segmentation: cellpose](#scrollTo=R7Zz3cKE6UMG)\n",
        "2.   [Automated corrections: Tracking cells in 3D with TrackMate in FIJI](#scrollTo=3-up0gcY_a9O)\n",
        "3.   [Manual segmentation](#scrollTo=IyW0d9L-lV3M)\n",
        "4.   [Refining the segmentation: Cellpose fine-tuning](#scrollTo=L7DxZhik4aQd)\n",
        "\n",
        "This notebook was inspired by Cellpose 2.0 notebook (https://github.com/MouseLand/cellpose) by Carsen Stringer et al. (https://mouseland.github.io/) and the Zero-Cost Deep-Learning to Enhance Microscopy project (https://github.com/HenriquesLab/DeepLearning_Collab/wiki)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvyuR08OZfw4"
      },
      "source": [
        "# 0. Installation setup\n",
        "\n",
        "We will first install cellpose and other dependencies, check the GPU is working, and mount google drive to get your models and images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNMQFs4NFXst"
      },
      "source": [
        "## Local installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJJwEILgFyJ-"
      },
      "source": [
        "Use the following instructions outside google collab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBBs-FbKFu9t"
      },
      "outputs": [],
      "source": [
        "# Create an environment with python 3.10.15\n",
        "conda create --name cellpose_3d python=3.10.15\n",
        "# Activate that environment\n",
        "conda activate cellpose_3d\n",
        "# Install cellpose with Graphical User Interface\n",
        "pip install cellpose[all]==3.1.0 matplotlib==3.7.3 plotly scikit-learn gdown notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFzLHe9cqEtu"
      },
      "source": [
        "Now, you can connect google colab to your own computer in case you have a GPU by following: https://research.google.com/colaboratory/local-runtimes.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRO-QzGhFaIt"
      },
      "source": [
        "## Colab installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUfSFlZgp1aV"
      },
      "source": [
        "Install cellpose -- by default the torch GPU version is installed in COLAB notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlMnqge-lQ9s"
      },
      "outputs": [],
      "source": [
        "!pip install cellpose[all]==3.1.0 matplotlib==3.7.3 plotly scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgqfXU7aFi3s"
      },
      "source": [
        "### All is working?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2cBEO1PLuO7"
      },
      "source": [
        "Check CUDA version and that GPU is working in cellpose and import other libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt8hgC7rniP8",
        "outputId": "d2e791e9-cadb-4f41-878d-a684c7e0bcfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvcc\n",
            "zsh:1: command not found: nvidia-smi\n",
            ">>> GPU activated? YES\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "\n",
        "import os, shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from cellpose import core, utils, io, models, metrics\n",
        "from glob import glob\n",
        "\n",
        "use_GPU = core.use_gpu()\n",
        "yn = ['NO', 'YES']\n",
        "print(f'>>> GPU activated? {yn[use_GPU]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heEiTSWQZZ6y"
      },
      "source": [
        "## Mount google drive\n",
        "\n",
        "Mount your google drive to access all your image files, segmentations, and custom models. This also ensures that any models you train are saved to your google drive. If you'd like to try out the notebook without your own files, please download the sample images provided (optional step in Setup below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGUNrjdRfVDs",
        "outputId": "3fa6b016-513f-46ca-a69e-7f9b85fa072b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@markdown ###Run this cell to connect your Google Drive to Colab\n",
        "\n",
        "#@markdown * Click on the URL.\n",
        "\n",
        "#@markdown * Sign in your Google Account.\n",
        "\n",
        "#@markdown * Copy the authorization code.\n",
        "\n",
        "#@markdown * Enter the authorization code.\n",
        "\n",
        "#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\".\n",
        "\n",
        "#mounts user's Google Drive to Google Colab.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1Ta76yatmjH"
      },
      "source": [
        "## Download sample images (optional)\n",
        "\n",
        "If you want to test this protocol with some sample images, run the next code. These images are described [here](https://www.ebi.ac.uk/bioimage-archive/galleries/S-BIAD843-ai.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC7BIPW06z4t"
      },
      "outputs": [],
      "source": [
        "# !rm -rf labelled_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMG3YYFSdieb",
        "outputId": "f7472c28-b893-4bac-8727-1d28f780c19a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1gXmuJlNYXLxAZypwEb2dhOs0t9LxL04p\n",
            "From (redirected): https://drive.google.com/uc?id=1gXmuJlNYXLxAZypwEb2dhOs0t9LxL04p&confirm=t&uuid=85dbc97c-91c9-40cb-9470-b06a6d0a3a27\n",
            "To: /Users/wei-tunghsu/Documents/GitHub/3D-deep-segmentation-protocol/labelled_data.tar.gz\n",
            "100%|██████████| 46.7M/46.7M [00:29<00:00, 1.60MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x labelled_data/\n",
            "x labelled_data/.DS_Store\n",
            "x labelled_data/segmented/\n",
            "x labelled_data/raw/\n",
            "x labelled_data/raw/WD3.2_21-03_WT_MP.tif\n",
            "x labelled_data/raw/WD1_15-02_WT_confocalonly.tif\n",
            "x labelled_data/raw/WD2.1_21-02_WT_confocalonly.tif\n",
            "x labelled_data/raw/WD1.1_17-03_WT_MP.tif\n",
            "x labelled_data/segmented/WD1.1_17-03_WT_MP_segmented.tif\n",
            "x labelled_data/segmented/WD3.2_21-03_WT_MP_segmented.tif\n",
            "x labelled_data/segmented/WD2.1_21-02_WT_confocalonly_segmented.tif\n",
            "x labelled_data/segmented/WD1_15-02_WT_confocalonly_segmented.tif\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "from natsort import natsorted\n",
        "\n",
        "!rm -rf labelled_data/\n",
        "\n",
        "# Download data from google drive\n",
        "url = 'https://drive.google.com/uc?id=1gXmuJlNYXLxAZypwEb2dhOs0t9LxL04p'\n",
        "gdown.download(url, 'labelled_data.tar.gz', quiet=False)\n",
        "\n",
        "!tar -xzvf labelled_data.tar.gz\n",
        "!rm labelled_data.tar.gz\n",
        "\n",
        "# Copy folder to 'initial_segmentation'\n",
        "!cp -r labelled_data/raw labelled_data/initial_segmentation/\n",
        "!cp -r labelled_data/raw labelled_data/denoised_raw/\n",
        "!cp -r labelled_data/raw labelled_data/improved_model/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7Zz3cKE6UMG"
      },
      "source": [
        "# 1. Initial segmentation: Cellpose\n",
        "\n",
        "Cellpose is a deep learning software that can segment cells in 2D and 3D.\n",
        "\n",
        "**We highly recommend to do the initial segmentation using the Cellpose graphical user interface (gui).**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiMy4cEqI1ON"
      },
      "source": [
        "## (LOCAL ONLY) Graphical User Interface (GUI) with a single image\n",
        "\n",
        "Open one of the 3D images and select the best parameters based on visual inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rsoAvcGlSip",
        "outputId": "0e2525dd-3ae3-46fd-e103-1d78c3da1c2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-16 11:34:06,726 [INFO] WRITING LOG OUTPUT TO /Users/wei-tunghsu/.cellpose/run.log\n",
            "2025-12-16 11:34:06,726 [INFO] \n",
            "cellpose version: \t3.1.0 \n",
            "platform:       \tdarwin \n",
            "python version: \t3.10.15 \n",
            "torch version:  \t2.8.0\n",
            "2025-12-16 11:34:07,260 [INFO] ** TORCH MPS version installed and working. **\n"
          ]
        }
      ],
      "source": [
        "!python -m cellpose --Zstack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkkR8R1WIv93"
      },
      "source": [
        "## Code to run with all images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vDu4Ixjo588O"
      },
      "outputs": [],
      "source": [
        "# model name and path\n",
        "\n",
        "# model name and path\n",
        "#@markdown ###Name of the pretrained model:\n",
        "from cellpose import models\n",
        "initial_model = \"cyto3\" #@param [\"cyto\", \"cyto3\",\"nuclei\",\"tissuenet_cp3\", \"livecell_cp3\", \"yeast_PhC_cp3\", \"yeast_BF_cp3\", \"bact_phase_cp3\", \"bact_fluor_cp3\", \"deepbacs_cp3\", \"scratch\"]\n",
        "\n",
        "#@markdown ###Path to images:\n",
        "\n",
        "input_dir = \"/Users/wei-tunghsu/Documents/GitHub/3D-deep-segmentation-protocol/labelled_data/initial_segmentation\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ###Channel Parameters:\n",
        "\n",
        "Channel_to_use_for_segmentation = \"Grayscale\" #@param [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
        "\n",
        "# Here we match the channel to number\n",
        "if Channel_to_use_for_segmentation == \"Grayscale\":\n",
        "  chan = 0\n",
        "elif Channel_to_use_for_segmentation == \"Blue\":\n",
        "  chan = 3\n",
        "elif Channel_to_use_for_segmentation == \"Green\":\n",
        "  chan = 2\n",
        "elif Channel_to_use_for_segmentation == \"Red\":\n",
        "  chan = 1\n",
        "\n",
        "#@markdown ### GPU (default) or CPU:\n",
        "\n",
        "use_GPU = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### Segmentation parameters:\n",
        "\n",
        "#@markdown Diameter of cells (set to zero to use diameter from training set):\n",
        "diameter =  60#@param {type:\"number\"}\n",
        "#@markdown Threshold on cellprob output to seed cell masks (set lower to include more pixels or higher to include fewer, e.g. in range from (-6, 6)):\n",
        "cellprob_threshold=0 #@param {type:\"slider\", min:-6, max:6, step:1}\n",
        "#@markdown Stitch 2D masks into a 3D volume using a stitch_threshold on IOU:\n",
        "stitch_threshold=0.05 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown Smooth flows with gaussian filter of this stddev\n",
        "dP_smooth=0.0 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown Volumetric stacks do not always have the same sampling in XY as they do in Z\n",
        "anisotropy=1.0 #@param {type:\"slider\", min:0, max:2, step:0.01}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpB5tFpJRXbv",
        "collapsed": true,
        "outputId": "51f9c635-03d5-4f67-85a2-686ab26c3085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python -m cellpose --use_gpu --save_tif --Zstack --verbose --dir /Users/wei-tunghsu/Documents/GitHub/3D-deep-segmentation-protocol/labelled_data/initial_segmentation --pretrained_model cyto3 --chan 0 --diameter 60 --stitch_threshold 0.05 --dP_smooth 0.0 --anisotropy 1.0\n",
            "2025-12-16 11:43:30,251 [INFO] WRITING LOG OUTPUT TO /Users/wei-tunghsu/.cellpose/run.log\n",
            "2025-12-16 11:43:30,251 [INFO] \n",
            "cellpose version: \t3.1.0 \n",
            "platform:       \tdarwin \n",
            "python version: \t3.10.15 \n",
            "torch version:  \t2.8.0\n",
            "2025-12-16 11:43:30,288 [INFO] ** TORCH MPS version installed and working. **\n",
            "2025-12-16 11:43:30,288 [INFO] >>>> using GPU (MPS)\n",
            "2025-12-16 11:43:30,291 [INFO] >>>> running cellpose on 4 images using chan_to_seg GRAY and chan (opt) NONE\n",
            "2025-12-16 11:43:30,291 [INFO] ** TORCH MPS version installed and working. **\n",
            "2025-12-16 11:43:30,291 [INFO] >>>> using GPU (MPS)\n",
            "2025-12-16 11:43:30,291 [INFO] >> cyto3 << model set to be used\n",
            "2025-12-16 11:43:30,382 [INFO] >>>> loading model /Users/wei-tunghsu/.cellpose/models/cyto3\n",
            "2025-12-16 11:43:30,475 [INFO] >>>> model diam_mean =  30.000 (ROIs rescaled to this size during training)\n",
            "2025-12-16 11:43:30,476 [INFO] >>>> using diameter 60.000 for all images\n",
            "2025-12-16 11:43:30,483 [INFO] 0%|          | 0/4 [00:00<?, ?it/s]\n",
            "2025-12-16 11:43:30,539 [INFO] reading tiff with 105 planes\n",
            "\n",
            "100%|███████████████████████████████████████| 105/105 [00:00<00:00, 4970.00it/s]\u001b[A\n",
            "2025-12-16 11:43:30,567 [INFO] channels set to [0, 0]\n",
            "2025-12-16 11:43:30,567 [INFO] ~~~ FINDING MASKS ~~~\n",
            "2025-12-16 11:43:30,567 [WARNING] z_axis not specified, assuming it is dim 0\n",
            "2025-12-16 11:43:30,567 [WARNING] if this is actually the channel_axis, use 'model.eval(channel_axis=0, ...)'\n",
            "2025-12-16 11:43:30,567 [INFO] multi-stack tiff read in as having 105 planes 1 channels\n",
            "2025-12-16 11:43:30,819 [INFO] \n",
            "2025-12-16 11:43:30,819 [INFO] 0%|          | 0/53 [00:00<?, ?it/s]\n",
            "2025-12-16 11:43:30,819 [INFO] \u001b[A\n",
            "2025-12-16 11:43:41,597 [INFO] 100%|##########| 53/53 [00:10<00:00,  4.92it/s]\n",
            "2025-12-16 11:43:41,746 [INFO] network run in 10.93s\n",
            "2025-12-16 11:43:41,747 [INFO] \n",
            "2025-12-16 11:43:41,747 [INFO] 0%|          | 0/105 [00:00<?, ?it/s]\n",
            "2025-12-16 11:43:41,747 [INFO] \u001b[A\n",
            "2025-12-16 11:43:42,017 [INFO] 0%|          | 0/105 [00:00<?, ?it/s]\n",
            "2025-12-16 11:43:42,017 [INFO] 0%|          | 0/4 [00:11<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/miniconda3/envs/nextflow_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/opt/miniconda3/envs/nextflow_env/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/opt/miniconda3/envs/nextflow_env/lib/python3.10/site-packages/cellpose/__main__.py\", line 358, in <module>\n",
            "    main()\n",
            "  File \"/opt/miniconda3/envs/nextflow_env/lib/python3.10/site-packages/cellpose/__main__.py\", line 205, in main\n",
            "    out = model.eval(\n",
            "  File \"/opt/miniconda3/envs/nextflow_env/lib/python3.10/site-packages/cellpose/models.py\", line 210, in eval\n",
            "    masks, flows, styles = self.cp.eval(x, channels=channels,\n",
            "  File \"/opt/miniconda3/envs/nextflow_env/lib/python3.10/site-packages/cellpose/models.py\", line 534, in eval\n",
            "    masks = self._compute_masks(x.shape, dP, cellprob, flow_threshold=flow_threshold,\n",
            "  File \"/opt/miniconda3/envs/nextflow_env/lib/python3.10/site-packages/cellpose/models.py\", line 620, in _compute_masks\n",
            "    outputs = dynamics.resize_and_compute_masks(\n",
            "  File \"/opt/miniconda3/envs/nextflow_env/lib/python3.10/site-packages/cellpose/dynamics.py\", line 839, in resize_and_compute_masks\n",
            "    mask = compute_masks(dP, cellprob, niter=niter,\n",
            "  File \"/opt/miniconda3/envs/nextflow_env/lib/python3.10/site-packages/cellpose/dynamics.py\", line 906, in compute_masks\n",
            "    mask = get_masks_torch(p_final, inds, dP.shape[1:], \n",
            "  File \"/opt/miniconda3/envs/nextflow_env/lib/python3.10/site-packages/cellpose/dynamics.py\", line 757, in get_masks_torch\n",
            "    coo = torch.sparse_coo_tensor(pt, torch.ones(pt.shape[1], device=pt.device, dtype=torch.int), \n",
            "NotImplementedError: Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseMPS' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_sparse_coo_tensor_with_dims_and_tensors' is only available for these backends: [MPS, Meta, SparseCPU, SparseMeta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradMAIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastMTIA, AutocastMAIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n",
            "\n",
            "MPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:79 [backend fallback]\n",
            "Meta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterMeta_0.cpp:13978 [kernel]\n",
            "SparseCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCPU_0.cpp:2598 [kernel]\n",
            "SparseMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseMeta_0.cpp:342 [kernel]\n",
            "BackendSelect: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterBackendSelect.cpp:792 [kernel]\n",
            "Python: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\n",
            "FuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\n",
            "Functionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:375 [backend fallback]\n",
            "Named: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\n",
            "Conjugate: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\n",
            "Negative: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\n",
            "ZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\n",
            "ADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:104 [backend fallback]\n",
            "AutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradMTIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradMAIA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "AutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_2.cpp:20142 [autograd kernel]\n",
            "Tracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:17887 [kernel]\n",
            "AutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\n",
            "AutocastMTIA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\n",
            "AutocastMAIA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:504 [backend fallback]\n",
            "AutocastXPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:542 [backend fallback]\n",
            "AutocastMPS: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\n",
            "AutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\n",
            "FuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\n",
            "BatchedNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\n",
            "FuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\n",
            "Batched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\n",
            "VmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
            "FuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\n",
            "PythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\n",
            "FuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\n",
            "PreDispatch: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\n",
            "PythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if use_GPU:\n",
        "  run_str = f'python -m cellpose --use_gpu --save_tif --Zstack --verbose --dir {input_dir} --pretrained_model {initial_model} --chan {chan} --diameter {diameter} --stitch_threshold {stitch_threshold} --dP_smooth {dP_smooth} --anisotropy {anisotropy} --cellprob_threshold {cellprob_threshold}'\n",
        "else:\n",
        "  run_str = f'python -m cellpose --save_tif --Zstack --verbose --dir {input_dir} --pretrained_model {initial_model} --chan {chan} --diameter {diameter} --stitch_threshold {stitch_threshold} --dP_smooth {dP_smooth} --anisotropy {anisotropy} --cellprob_threshold {cellprob_threshold}'\n",
        "print(run_str)\n",
        "!$run_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT1rmaHNdGAJ"
      },
      "source": [
        "### Visualising images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp5hy7dzf6bV",
        "outputId": "5ce2417f-efa3-4dd7-f2c8-9026237aee16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105/105 [00:00<00:00, 9733.28it/s]\n",
            "100%|██████████| 36/36 [00:00<00:00, 7201.21it/s]\n",
            "100%|██████████| 38/38 [00:00<00:00, 10122.16it/s]\n",
            "100%|██████████| 60/60 [00:00<00:00, 8313.78it/s]\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Users/wei-tunghsu/Documents/GitHub/3D-deep-segmentation-protocol/labelled_data/initial_segmentation/WD1.1_17-03_WT_MP_cp_masks.tif'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/b3/8ml60pdd7n75jd6sbtf91p700000gn/T/ipykernel_85667/2346554689.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Get images and masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cp_masks'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.tif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cp_masks.tif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mvisualize_3d_sections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/b3/8ml60pdd7n75jd6sbtf91p700000gn/T/ipykernel_85667/2346554689.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_3d_sections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmented\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \"\"\"\n\u001b[1;32m     49\u001b[0m     \u001b[0mVisualizes\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0msections\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0mD\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mits\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/miniconda3/envs/nextflow_env/lib/python3.10/site-packages/cellpose/io.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \"\"\"\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# ensure that extension check is not case sensitive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".tif\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".tiff\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".flex\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtifffile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTiffFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtif\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mltif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtif\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mfull_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtif\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshaped_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/miniconda3/envs/nextflow_env/lib/python3.10/site-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, file, mode, name, offset, size, omexml, _multifile, _useframes, _parent, **is_flags)\u001b[0m\n\u001b[1;32m   4231\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid OME-XML'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4232\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_omexml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0momexml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4235\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4237\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multifile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_multifile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_multifile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/miniconda3/envs/nextflow_env/lib/python3.10/site-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, file, mode, name, offset, size)\u001b[0m\n\u001b[1;32m  14607\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14608\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14609\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14610\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNullContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 14611\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  14612\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/miniconda3/envs/nextflow_env/lib/python3.10/site-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  14626\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r+b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'xb'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14627\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'invalid mode {self._mode}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14628\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 14630\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  14631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  14633\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileHandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/wei-tunghsu/Documents/GitHub/3D-deep-segmentation-protocol/labelled_data/initial_segmentation/WD1.1_17-03_WT_MP_cp_masks.tif'"
          ]
        }
      ],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from cellpose import io\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "\n",
        "def visualize_3d_sections(image, masks, segmented=True, num_sections=3):\n",
        "    \"\"\"\n",
        "    Visualizes different random sections of the 3D image and its segmentation.\n",
        "\n",
        "    Args:\n",
        "        image: A 3D numpy array representing the image.\n",
        "        masks: A 3D numpy array representing the cell masks.\n",
        "        num_sections: The number of random sections to visualize.\n",
        "    \"\"\"\n",
        "\n",
        "    z_dim = image.shape[0]\n",
        "\n",
        "    # Generate 'num_sections' random numbers\n",
        "    random_sections = np.random.randint(0, z_dim, num_sections)\n",
        "\n",
        "    # Sort the random numbers in ascending order\n",
        "    random_sections = np.sort(random_sections)\n",
        "\n",
        "    # Create a colormap for all the sections\n",
        "    cmap = matplotlib.colormaps.get_cmap('prism')\n",
        "\n",
        "    for id in range(num_sections):\n",
        "        z_slice = random_sections[id]\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(image[z_slice], cmap='gray')\n",
        "        plt.title(f\"Image - Z Slice: {z_slice}\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        if segmented:\n",
        "          plt.imshow(masks[z_slice], cmap)\n",
        "          plt.title(f\"Segmentation - Z Slice: {z_slice}\")\n",
        "        else:\n",
        "          plt.imshow(masks[z_slice], cmap='gray')\n",
        "          plt.title(f\"Image - Z Slice: {z_slice}\")\n",
        "        plt.show()\n",
        "\n",
        "# Get images and masks\n",
        "files = io.get_image_files(input_dir, '_cp_masks')\n",
        "images = [io.imread(f) for f in files]\n",
        "masks = [io.imread(f.replace('.tif', '_cp_masks.tif')) for f in files]\n",
        "visualize_3d_sections(images[3], masks[3], num_sections=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg62dH2zsoXt"
      },
      "source": [
        "## Segmentation evaluation metrics: biology-based\n",
        "Common metrics used to evaluate the quality of a segmentation are: IoU (Intersection over Union), Dice coefficient, ... However, these metrics are not always suitable for biological images. We have developed a new metric that is more biologically relevant. It also helped us to improve the segmentation results from our manual annotations.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5LwS0FwAcw6"
      },
      "outputs": [],
      "source": [
        "from cellpose import io\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "def calculate_cell_persistence_score(mask_img, min_percentage=85):\n",
        "  \"\"\"\n",
        "  Count the number of cells that are present in a 'min_percentage' of slices.\n",
        "  Developed by Giulia Paci\n",
        "  @mask_img: 3D mask image\n",
        "  @min_percentage: minimum percentage of slices that a cell must be present in to be considered a good cell\n",
        "  return: number of good cells and number of bad cells\n",
        "  \"\"\"\n",
        "  z_planes = mask_img.shape[0]\n",
        "\n",
        "  # Minimum of number of slices for a cell to be correct\n",
        "  target_n_planes = (min_percentage / 100 ) * z_planes\n",
        "  #print(f'Minimum number of z-planes is: {target_n_planes}')\n",
        "\n",
        "  # Count the number of good cells\n",
        "  unique_ids = np.unique(mask_img)\n",
        "  count_good = 0\n",
        "  count_bad = 0\n",
        "\n",
        "  # Loop\n",
        "  for cell_id in unique_ids:\n",
        "    if cell_id == 0:\n",
        "      continue\n",
        "\n",
        "    # Get the voxels of the current cell\n",
        "    current_img = mask_img == cell_id\n",
        "\n",
        "    # Get the position of the voxels\n",
        "    binary_img_pos = np.where(current_img)\n",
        "\n",
        "    # Check if they are connected by using connected components\n",
        "    _, num_objects = ndimage.label(current_img)\n",
        "\n",
        "    if num_objects > 2:\n",
        "      count_bad = count_bad + 1\n",
        "      continue\n",
        "\n",
        "    # Get only the unique Z position of the voxels\n",
        "    unique_z_position = np.unique(binary_img_pos[0])\n",
        "\n",
        "    # Count the number of slices that the cell is present in\n",
        "    if len(unique_z_position) > target_n_planes:\n",
        "        count_good = count_good + 1\n",
        "    else:\n",
        "        count_bad = count_bad + 1\n",
        "\n",
        "  return count_good, count_bad\n",
        "\n",
        "\n",
        "# Get evaluation of segmentation\n",
        "files = io.get_image_files(input_dir, '_cp_masks')\n",
        "for file in files:\n",
        "  print(f'File name: {file}')\n",
        "  mask = io.imread(file.replace('.tif', '_cp_masks.tif'))\n",
        "  good_cells, bad_cells = calculate_cell_persistence_score(mask)\n",
        "  print(f'Number of good cells: {good_cells} and bad cells: {bad_cells}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr05EylcMAyI"
      },
      "source": [
        "###Comparison 0: Cyto3 without extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf3ajz7eMACh"
      },
      "outputs": [],
      "source": [
        "!cp -r labelled_data/raw labelled_data/initial_segmentation_only_cyto3/\n",
        "input_dir = \"labelled_data/initial_segmentation_only_cyto3\"\n",
        "if use_GPU:\n",
        "  run_str = f'python -m cellpose --use_gpu --save_tif --Zstack --verbose --dir {input_dir} --pretrained_model cyto3 --chan 0 --diameter {diameter}'\n",
        "else:\n",
        "  run_str = f'python -m cellpose --save_tif --Zstack --verbose --dir {input_dir} --pretrained_model cyto3 --chan 0 --diameter {diameter}'\n",
        "print(run_str)\n",
        "!$run_str\n",
        "\n",
        "# Get evaluation of segmentation\n",
        "files = io.get_image_files(input_dir, '_cp_masks')\n",
        "for file in files:\n",
        "  print(f'File name: {file}')\n",
        "  img = io.imread(file)\n",
        "  mask = io.imread(file.replace('.tif', '_cp_masks.tif'))\n",
        "  good_cells, bad_cells = calculate_cell_persistence_score(mask)\n",
        "  print(f'Number of good cells: {good_cells} and bad cells: {bad_cells}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wri_gk9t4aH"
      },
      "source": [
        "### Comparison 1: Stitch threshold 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUGRb3tOt4rj"
      },
      "outputs": [],
      "source": [
        "!cp -r labelled_data/raw labelled_data/initial_segmentation_stitch_0_5/\n",
        "input_dir = \"labelled_data/initial_segmentation_stitch_0_5\"\n",
        "if use_GPU:\n",
        "  run_str = f'python -m cellpose --use_gpu --save_tif --Zstack --verbose --dir {input_dir} --pretrained_model cyto3 --chan 0 --diameter {diameter} --stitch_threshold 0.5'\n",
        "else:\n",
        "  run_str = f'python -m cellpose --save_tif --Zstack --verbose --dir {input_dir} --pretrained_model cyto3 --chan 0 --diameter {diameter} --stitch_threshold 0.5'\n",
        "print(run_str)\n",
        "!$run_str\n",
        "\n",
        "# Get evaluation of segmentation\n",
        "files = io.get_image_files(input_dir, '_cp_masks')\n",
        "for file in files:\n",
        "  print(f'File name: {file}')\n",
        "  img = io.imread(file)\n",
        "  mask = io.imread(file.replace('.tif', '_cp_masks.tif'))\n",
        "  good_cells, bad_cells = calculate_cell_persistence_score(mask)\n",
        "  print(f'Number of good cells: {good_cells} and bad cells: {bad_cells}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Xiprnqtzzu"
      },
      "source": [
        "### Comparison 2: Final best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiLO4Tlstsfn"
      },
      "outputs": [],
      "source": [
        "!cp -r labelled_data/raw labelled_data/initial_segmentation_stitch_0_0_5/\n",
        "input_dir = \"labelled_data/initial_segmentation_stitch_0_0_5\"\n",
        "if use_GPU:\n",
        "  run_str = f'python -m cellpose --use_gpu --save_tif --Zstack --verbose --dir {input_dir} --pretrained_model cyto3 --chan 0 --diameter {diameter} --stitch_threshold 0.05'\n",
        "else:\n",
        "  run_str = f'python -m cellpose --save_tif --Zstack --verbose --dir {input_dir} --pretrained_model cyto3 --chan 0 --diameter {diameter} --stitch_threshold 0.05'\n",
        "\n",
        "print(run_str)\n",
        "!$run_str\n",
        "\n",
        "# Get evaluation of segmentation\n",
        "files = io.get_image_files(input_dir, '_cp_masks')\n",
        "for file in files:\n",
        "  print(f'File name: {file}')\n",
        "  img = io.imread(file)\n",
        "  mask = io.imread(file.replace('.tif', '_cp_masks.tif'))\n",
        "  good_cells, bad_cells = calculate_cell_persistence_score(mask)\n",
        "  print(f'Number of good cells: {good_cells} and bad cells: {bad_cells}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKAnyLksbD_2"
      },
      "source": [
        "### Comparing Cell Persistence Score, DICE and Intersection Over Union\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77L8rsBkbLLA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def calculate_metrics(pred, gt, pred_id, gt_id):\n",
        "    pred_region = (pred == pred_id)\n",
        "    gt_region = (gt == gt_id)\n",
        "    pred_region_num_pixels = len(np.where(pred_region)[0])\n",
        "    gt_region_num_pixels = len(np.where(gt_region)[0])\n",
        "    intersection = len(np.where(pred_region & gt_region)[0])\n",
        "    union = len(np.where(pred_region | gt_region)[0])\n",
        "    dice = (2 * intersection) / (pred_region_num_pixels + gt_region_num_pixels) if (pred_region_num_pixels + gt_region_num_pixels) > 0 else 0\n",
        "    iou = intersection / union if union > 0 else 0\n",
        "    return dice, iou\n",
        "\n",
        "def evaluate_3d_instances(pred, gt, iou_thresh=0.5):\n",
        "\n",
        "    pred_ids = set(np.unique(pred)) - {0}\n",
        "    gt_ids = set(np.unique(gt)) - {0}\n",
        "\n",
        "    tp, fp, fn = 0, 0, 0\n",
        "    matched_pred = set()\n",
        "    instance_ious = []\n",
        "    instance_dices = []\n",
        "\n",
        "    for gt_id in gt_ids:\n",
        "      best_iou, best_dice, best_pred = 0, 0, None\n",
        "      for pred_id in pred_ids:\n",
        "          dice, iou = calculate_metrics(pred, gt, pred_id, gt_id)\n",
        "          if iou > best_iou:\n",
        "              best_iou, best_dice, best_pred = iou, dice, pred_id\n",
        "\n",
        "      instance_ious.append(best_iou)\n",
        "      instance_dices.append(best_dice)\n",
        "\n",
        "      if best_iou >= iou_thresh and best_pred not in matched_pred:\n",
        "          tp += 1\n",
        "          matched_pred.add(best_pred)\n",
        "\n",
        "    fp = len(pred_ids) - len(matched_pred)\n",
        "    fn = len(gt_ids) - tp\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    iou = np.mean(instance_ious) if instance_ious else 0\n",
        "    dice = np.mean(instance_dices) if instance_dices else 0\n",
        "\n",
        "    return precision, recall, f1, iou, dice\n",
        "\n",
        "# Plot with the following folders\n",
        "input_folders = ['labelled_data/initial_segmentation_only_cyto3', 'labelled_data/initial_segmentation_stitch_0_5', 'labelled_data/initial_segmentation_stitch_0_0_5'] #'labelled_data/manual_corrections_2d', 'labelled_data/trackmate',\n",
        "ground_truth_folder = 'labelled_data/segmented'\n",
        "\n",
        "for input_folder in input_folders:\n",
        "  files = io.get_image_files(input_folder, '_cp_masks')\n",
        "  gt_files = io.get_image_files(ground_truth_folder, '.tif')\n",
        "  for file, gt_file in zip(files, gt_files):\n",
        "      print(f'File name: {file}')\n",
        "      print(f'GT file name: {gt_file}')\n",
        "      mask = io.imread(file.replace('.tif', '_cp_masks.tif'))\n",
        "      good_cells, bad_cells = calculate_cell_persistence_score(mask)\n",
        "      print(f'Number of good cells: {good_cells} and bad cells: {bad_cells}')\n",
        "      precision, recall, f1, iou, dice = evaluate_3d_instances(mask, io.imread(gt_file))\n",
        "      print(f'Precision: {precision}, Recall: {recall}, F1: {f1}, IOU: {iou}, DICE: {dice}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SZ_PDQvxLvo"
      },
      "source": [
        "### Ground truth\n",
        "\n",
        "We have segmented our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjNBWTqAxRYu"
      },
      "outputs": [],
      "source": [
        "labelled_data = \"labelled_data/segmented\"\n",
        "files = io.get_image_files(labelled_data, '_cp_masks')\n",
        "\n",
        "# Get evaluation of segmentation\n",
        "for file in files:\n",
        "  print(f'File name: {file}')\n",
        "  mask = io.imread(file)\n",
        "  good_cells, bad_cells = calculate_cell_persistence_score(mask)\n",
        "  print(f'Number of good cells: {good_cells} and bad cells: {bad_cells}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-up0gcY_a9O"
      },
      "source": [
        "# 2. Automated corrections: Tracking cells in 3D with TrackMate in FIJI\n",
        "We still had issues with our segmentation results, especially when cells were touching each other. Therefore, we used TrackMate to track cells in 3D and then stitched the cells together. This allowed us to obtain a more accurate segmentation of the cells. The process is explained in the manuscript associated with this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyW0d9L-lV3M"
      },
      "source": [
        "# 3. Manual segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jLF3CZAlkAI"
      },
      "source": [
        "Follow: https://napari.org/stable/tutorials/fundamentals/installation.html#napari-installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwaQNye6lZZF"
      },
      "outputs": [],
      "source": [
        "!conda create -y -n napari-env -c conda-forge python=3.10\n",
        "!conda activate napari-env\n",
        "!python -m pip install \"napari[all]\"\n",
        "!python -m pip install devbio-napari\n",
        "!napari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7DxZhik4aQd"
      },
      "source": [
        "# 4. Refining the segmentation: Cellpose fine-tuning\n",
        "Cellpose fine-tuning is a feature that allows you to improve the segmentation results by training a new model on your own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL7nKT4u5cPR"
      },
      "source": [
        "### From 3D images to 2D images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1qBJXePXD9B"
      },
      "outputs": [],
      "source": [
        "# Code developed by Alvaro Miranda de Larra and Pablo Vicente-Munuera\n",
        "import os\n",
        "import tifffile as tiff\n",
        "\n",
        "def iterative_image_splicer(input_dir, output_dir, segmented_input=False):\n",
        "\n",
        "  # Create the output directory if it doesn't exist\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "  # Get a list of all .tif files in the input directory\n",
        "  tif_files = [f for f in os.listdir(input_dir) if f.endswith('.tif')]\n",
        "\n",
        "  # Iterate over each 3D image\n",
        "  for tif_file in tif_files:\n",
        "    if not tif_file.startswith('.'):\n",
        "      # Load the multi-directory TIFF image\n",
        "      with tiff.TiffFile(os.path.join(input_dir, tif_file)) as tif:\n",
        "          image = tif.asarray()\n",
        "\n",
        "      # Get the shape of the 3D image\n",
        "      z, y, x = image.shape\n",
        "      print(image.shape)\n",
        "      # Generate 2D images along XY, XZ, and YZ coordinates\n",
        "\n",
        "      for z_coord in range(z):\n",
        "          xy_image = image[z_coord, :, :]  # XY plane at the current Z coordinate\n",
        "\n",
        "          # Save the 2D images with appropriate names\n",
        "          base_name = os.path.splitext(tif_file)[0]\n",
        "          # Remove '_segmented' from base_name\n",
        "          base_name = base_name.replace('_segmented', '')\n",
        "          if segmented_input:\n",
        "            tiff.imwrite(os.path.join(output_dir, f'{base_name}_XY_Z{z_coord}_masks.tif'), xy_image)\n",
        "          else:\n",
        "            tiff.imwrite(os.path.join(output_dir, f'{base_name}_XY_Z{z_coord}.tif'), xy_image)\n",
        "\n",
        "      for xy_coord in range(x):\n",
        "          xz_image = image[:, :, xy_coord]  # XZ plane at the current Y coordinate\n",
        "          yz_image = image[:, xy_coord, :]  # YZ plane at the current X coordinate\n",
        "\n",
        "          if segmented_input:\n",
        "            tiff.imwrite(os.path.join(output_dir, f'{base_name}_XZ_Y{xy_coord}_masks.tif'), xz_image)\n",
        "            tiff.imwrite(os.path.join(output_dir, f'{base_name}_YZ_X{xy_coord}_masks.tif'), yz_image)\n",
        "          else:\n",
        "            tiff.imwrite(os.path.join(output_dir, f'{base_name}_XZ_Y{xy_coord}.tif'), xz_image)\n",
        "            tiff.imwrite(os.path.join(output_dir, f'{base_name}_YZ_X{xy_coord}.tif'), yz_image)\n",
        "\n",
        "!rm -rf labelled_data_2D/\n",
        "iterative_image_splicer('labelled_data/raw/', 'labelled_data_2D')\n",
        "iterative_image_splicer('labelled_data/segmented/', 'labelled_data_2D', segmented_input=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j4m33Fuw5vm"
      },
      "source": [
        "what the training images look like + their labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldNwr_zxMVha"
      },
      "outputs": [],
      "source": [
        "from natsort import natsorted\n",
        "from glob import glob\n",
        "\n",
        "train_files = natsorted([f for f in glob('labelled_data_2D/*.tif')\n",
        "                        if '_masks' not in f])\n",
        "train_seg = natsorted(glob('labelled_data_2D/*_masks.tif'))\n",
        "\n",
        "num_images_to_show = 5\n",
        "\n",
        "# Generate 'num_sections' random numbers\n",
        "random_sections = np.random.randint(0, len(train_files), num_images_to_show)\n",
        "\n",
        "# Visualize a few training and segmentation images\n",
        "for k,f in enumerate(random_sections):\n",
        "    img = io.imread(train_files[f])\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(f\"Image - name: {train_files[f]}\")\n",
        "\n",
        "    # Get the corresponding segmentation image\n",
        "    seg = io.imread(train_seg[f])\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(seg, cmap='prism')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKOhEhZHkVpZ"
      },
      "source": [
        "### Split into training and set test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAbu2H2Xh8Uo"
      },
      "outputs": [],
      "source": [
        "!rm -rf test/\n",
        "!rm -rf train/\n",
        "\n",
        "# Divide sets into training and set test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_files = natsorted([f for f in glob('labelled_data_2D/*.tif')\n",
        "                        if '_masks.tif' not in f])\n",
        "train_files, test_files = train_test_split(train_files, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save files from train into 'train'\n",
        "os.makedirs('train', exist_ok=True)\n",
        "for f in train_files:\n",
        "    shutil.copy(f, 'train')\n",
        "    # Get the '_mask' from the file f\n",
        "    shutil.copy(f.replace('.tif', '_masks.tif'), 'train')\n",
        "\n",
        "# Save files from test into 'test'\n",
        "os.makedirs('test', exist_ok=True)\n",
        "for f in test_files:\n",
        "    shutil.copy(f, 'test')\n",
        "    # Get the '_mask' from the file f\n",
        "    shutil.copy(f.replace('.tif', '_masks.tif'), 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfE75htF0l84"
      },
      "source": [
        "### Train model on manual annotations\n",
        "\n",
        "Skip this step if you already have a pretrained model.\n",
        "\n",
        "Fill out the form below with the paths to your data and the parameters to start training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLdKNWQ4jxy5"
      },
      "source": [
        "### Training parameters\n",
        "\n",
        "<font size = 4> **Paths for training, predictions and results**\n",
        "\n",
        "\n",
        "<font size = 4>**`train_dir:`, `test_dir`:** These are the paths to your folders train_dir (with images and masks of training images) and test_dir (with images and masks of test images). You can leave the test_dir blank, but it's recommended to have some test images to check the model's performance. To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n",
        "\n",
        "<font size = 4>**`initial_model`:** Choose a model from the cellpose [model zoo](https://cellpose.readthedocs.io/en/latest/models.html#model-zoo) to start from.\n",
        "\n",
        "<font size = 4>**`model_name`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
        "\n",
        "<font size = 4>**Training parameters**\n",
        "\n",
        "<font size = 4>**`number_of_epochs`:** Input how many epochs (rounds) the network will be trained. At least 100 epochs are recommended, but sometimes 250 epochs are necessary, particularly from scratch. **Default value: 100**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQI4aUxCjz3n",
        "outputId": "ae98eb20-3659-4acb-8ae0-08ba25c6753f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Default advanced parameters enabled\n"
          ]
        }
      ],
      "source": [
        "#@markdown ###Path to images and masks:\n",
        "\n",
        "train_dir = \"train\" #@param {type:\"string\"}\n",
        "test_dir = \"test\" #@param {type:\"string\"}\n",
        "#Define where the patch file will be saved\n",
        "base = \"/content\"\n",
        "\n",
        "# model name and path\n",
        "#@markdown ###Name of the pretrained model to start from and new model name:\n",
        "from cellpose import models\n",
        "initial_model = \"cyto3\" #@param [\"cyto\", \"cyto3\",\"nuclei\",\"tissuenet_cp3\", \"livecell_cp3\", \"yeast_PhC_cp3\", \"yeast_BF_cp3\", \"bact_phase_cp3\", \"bact_fluor_cp3\", \"deepbacs_cp3\", \"scratch\"]\n",
        "model_name = \"CP_improved_100\" #@param {type:\"string\"}\n",
        "use_GPU = True #@param {type:\"boolean\"}\n",
        "\n",
        "# other parameters for training.\n",
        "#@markdown ###Training Parameters:\n",
        "#@markdown Number of epochs:\n",
        "n_epochs =  100#@param {type:\"number\"}\n",
        "\n",
        "Channel_to_use_for_training = \"Grayscale\" #@param [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
        "\n",
        "# @markdown ###If you have a secondary channel that can be used for training, for instance nuclei, choose it here:\n",
        "\n",
        "Second_training_channel= \"None\" #@param [\"None\", \"Blue\", \"Green\", \"Red\"]\n",
        "\n",
        "\n",
        "#@markdown ###Advanced Parameters\n",
        "\n",
        "Use_Default_Advanced_Parameters = True #@param {type:\"boolean\"}\n",
        "#@markdown ###If not, please input:\n",
        "learning_rate = 0.1 #@param {type:\"number\"}\n",
        "weight_decay = 0.0001 #@param {type:\"number\"}\n",
        "\n",
        "if (Use_Default_Advanced_Parameters):\n",
        "  print(\"Default advanced parameters enabled\")\n",
        "  learning_rate = 0.1\n",
        "  weight_decay = 0.0001\n",
        "\n",
        "#here we check that no model with the same name already exist, if so delete\n",
        "model_path = train_dir + 'models/'\n",
        "if os.path.exists(model_path+'/'+model_name):\n",
        "  print(\"!! WARNING: \"+model_name+\" already exists and will be deleted in the following cell !!\")\n",
        "\n",
        "if len(test_dir) == 0:\n",
        "  test_dir = None\n",
        "\n",
        "# Here we match the channel to number\n",
        "if Channel_to_use_for_training == \"Grayscale\":\n",
        "  chan = 0\n",
        "elif Channel_to_use_for_training == \"Blue\":\n",
        "  chan = 3\n",
        "elif Channel_to_use_for_training == \"Green\":\n",
        "  chan = 2\n",
        "elif Channel_to_use_for_training == \"Red\":\n",
        "  chan = 1\n",
        "\n",
        "\n",
        "if Second_training_channel == \"Blue\":\n",
        "  chan2 = 3\n",
        "elif Second_training_channel == \"Green\":\n",
        "  chan2 = 2\n",
        "elif Second_training_channel == \"Red\":\n",
        "  chan2 = 1\n",
        "elif Second_training_channel == \"None\":\n",
        "  chan2 = 0\n",
        "\n",
        "if initial_model=='scratch':\n",
        "  initial_model = 'None'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JRxBPmatrK7"
      },
      "source": [
        "### Train new model\n",
        "\n",
        "Using settings from form above, train model in notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQsv-Iz7m_CF"
      },
      "outputs": [],
      "source": [
        "if use_GPU:\n",
        "  run_str = f'python -m cellpose --use_gpu --verbose --train --dir {train_dir} --pretrained_model {initial_model} --chan {chan} --n_epochs {n_epochs} --learning_rate {learning_rate} --weight_decay {weight_decay} --model_name_out {model_name}'\n",
        "else:\n",
        "  run_str = f'python -m cellpose --verbose --train --dir {train_dir} --pretrained_model {initial_model} --chan {chan} --n_epochs {n_epochs} --learning_rate {learning_rate} --weight_decay {weight_decay} --model_name_out {model_name}'\n",
        "\n",
        "if test_dir is not None:\n",
        "    run_str += f' --test_dir {test_dir}'\n",
        "print(run_str)\n",
        "!$run_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdH0j8-L6FuB"
      },
      "source": [
        "## Evaluate on test data (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zheUcb318c1B"
      },
      "outputs": [],
      "source": [
        "# model name and path\n",
        "\n",
        "# model name and path\n",
        "#@markdown ###Name of the pretrained model:\n",
        "from cellpose import models\n",
        "initial_model = \"CP_improved_100\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ###Path to images:\n",
        "\n",
        "input_dir = \"labelled_data/improved_model\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ###Channel Parameters:\n",
        "\n",
        "Channel_to_use_for_segmentation = \"Grayscale\" #@param [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
        "\n",
        "# Here we match the channel to number\n",
        "if Channel_to_use_for_segmentation == \"Grayscale\":\n",
        "  chan = 0\n",
        "elif Channel_to_use_for_segmentation == \"Blue\":\n",
        "  chan = 3\n",
        "elif Channel_to_use_for_segmentation == \"Green\":\n",
        "  chan = 2\n",
        "elif Channel_to_use_for_segmentation == \"Red\":\n",
        "  chan = 1\n",
        "\n",
        "#@markdown ### Segmentation parameters:\n",
        "\n",
        "#@markdown Diameter of cells (set to zero to use diameter from training set):\n",
        "diameter =  60#@param {type:\"number\"}\n",
        "#@markdown Threshold on cellprob output to seed cell masks (set lower to include more pixels or higher to include fewer, e.g. in range from (-6, 6)):\n",
        "cellprob_threshold=0 #@param {type:\"slider\", min:-6, max:6, step:1}\n",
        "#@markdown Stitch 2D masks into a 3D volume using a stitch_threshold on IOU:\n",
        "stitch_threshold=0.05 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown Smooth flows with gaussian filter of this stddev\n",
        "dP_smooth=0.0 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown Volumetric stacks do not always have the same sampling in XY as they do in Z\n",
        "anisotropy=1.0 #@param {type:\"slider\", min:0, max:2, step:0.01}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEn4jTkW8KC5"
      },
      "outputs": [],
      "source": [
        "if use_GPU:\n",
        "  run_str = f'python -m cellpose --use_gpu --save_tif --Zstack --verbose --dir {input_dir} --pretrained_model train/models/{initial_model} --chan {chan} --diameter {diameter} --stitch_threshold {stitch_threshold} --dP_smooth {dP_smooth} --anisotropy {anisotropy} --cellprob_threshold {cellprob_threshold}'\n",
        "else:\n",
        "  run_str = f'python -m cellpose --save_tif --Zstack --verbose --dir {input_dir} --pretrained_model train/models/{initial_model} --chan {chan} --diameter {diameter} --stitch_threshold {stitch_threshold} --dP_smooth {dP_smooth} --anisotropy {anisotropy} --cellprob_threshold {cellprob_threshold}'\n",
        "\n",
        "print(run_str)\n",
        "!$run_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73wx0H6FEbfE"
      },
      "outputs": [],
      "source": [
        "# Get evaluation of segmentation\n",
        "files = io.get_image_files(input_dir, '_cp_masks')\n",
        "for file in files:\n",
        "  print(f'File name: {file}')\n",
        "  mask = io.imread(file.replace('.tif', '_cp_masks.tif'))\n",
        "  good_cells, bad_cells = calculate_cell_persistence_score(mask)\n",
        "  print(f'Number of good cells: {good_cells} and bad cells: {bad_cells}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "IvyuR08OZfw4",
        "UNMQFs4NFXst",
        "mRO-QzGhFaIt",
        "heEiTSWQZZ6y",
        "d1Ta76yatmjH",
        "R7Zz3cKE6UMG",
        "AiMy4cEqI1ON",
        "YkkR8R1WIv93",
        "mT1rmaHNdGAJ",
        "jg62dH2zsoXt",
        "rr05EylcMAyI",
        "-wri_gk9t4aH",
        "n8Xiprnqtzzu",
        "vKAnyLksbD_2",
        "9SZ_PDQvxLvo",
        "3-up0gcY_a9O",
        "IyW0d9L-lV3M",
        "L7DxZhik4aQd",
        "iL7nKT4u5cPR",
        "CKOhEhZHkVpZ",
        "lLdKNWQ4jxy5",
        "3JRxBPmatrK7",
        "cdH0j8-L6FuB"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "nextflow_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}